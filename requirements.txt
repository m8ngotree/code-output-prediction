openai>=1.0.0
pyyaml>=6.0

# HuggingFace ecosystem for model training
transformers>=4.35.0
datasets>=2.14.0
accelerate>=0.24.0
peft>=0.6.0  # Parameter-efficient fine-tuning (LoRA, etc.)
bitsandbytes>=0.41.0  # 4-bit/8-bit quantization

# Training and optimization
torch>=2.0.0
trl>=0.7.0  # Transformer Reinforcement Learning

# Data and evaluation
numpy>=1.24.0
pandas>=2.0.0
scikit-learn>=1.3.0

# Utilities
tqdm>=4.65.0
wandb>=0.15.0  # Experiment tracking (optional)